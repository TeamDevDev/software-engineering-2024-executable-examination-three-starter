{"amount_correct": 18, "percentage_score": 28, "report_time": "2024-12-11 12:44:03", "checks": [{"description": "Ensure that the README.md file exists inside of the root of the GitHub repository", "check": "ConfirmFileExists", "status": true, "path": "../README.md"}, {"description": "Delete the phrase 'Add Your Name Here' and add your own name as an Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "Add Your Name Here", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 2 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for README.md", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 5 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Retype the every word in the Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "I adhered to the Allegheny College Honor Code while completing this executable examination.", "count": 3, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 2 fragment(s) in the README.md or the output while expecting exactly 3"}, {"description": "Indicate that you have completed all of the tasks in the README.md", "check": "MatchFileFragment", "options": {"fragment": "- [X]", "count": 11, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 0 fragment(s) in the README.md or the output while expecting exactly 11"}, {"description": "Ensure that question_one.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_one.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_one.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_one.py", "diagnostic": "Found 11 fragment(s) in the question_one.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_one.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 10, "exact": false}, "status": true, "path": "questions/question_one.py"}, {"description": "Create a sufficient number of single-line comments in question_one.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_one.py"}, {"description": "Ensure that test_question_one.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_one.py"}, {"description": "Ensure that question_two.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_two.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_two.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_two.py", "diagnostic": "Found 11 fragment(s) in the question_two.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_two.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 8, "exact": false}, "status": false, "path": "questions/question_two.py", "diagnostic": "Found 1 comment(s) in the question_two.py or the output"}, {"description": "Create a sufficient number of single-line comments in question_two.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_two.py"}, {"description": "Ensure that test_question_two.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_two.py"}, {"description": "Ensure that question_three.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_three.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_three.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_three.py", "diagnostic": "Found 13 fragment(s) in the question_three.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_three.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 8, "exact": false}, "status": false, "path": "questions/question_three.py", "diagnostic": "Found 2 comment(s) in the question_three.py or the output"}, {"description": "Create a sufficient number of single-line comments in question_three.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_three.py"}, {"description": "Ensure that test_question_two.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_three.py"}, {"description": "Ensure that question_four.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_four.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_four.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_four.py", "diagnostic": "Found 13 fragment(s) in the question_four.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_four.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 10, "exact": false}, "status": true, "path": "questions/question_four.py"}, {"description": "Create a sufficient number of single-line comments in question_four.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_four.py"}, {"description": "Ensure that test_question_four.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_four.py"}, {"description": "Run checks for Question 1 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Automatically analyze test coverage reports through set-theoretic approaches."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f9bfae7b1d0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_coverage_difference - AssertionError: Failed on case with identical \n     coverage reports\n     FAILED tests/test_question_one.py::test_compute_coverage_union - AssertionError: union should contain 4 items\n     \n     test_question_one.py::test_compute_coverage_difference\n       - Status: Failed\n         Line: 30\n         Exact: [] == [CoverageItem...covered=True)] ...\n         Message: Failed on case with identical coverage reports\n     \n     test_question_one.py::test_compute_coverage_union\n       - Status: Failed\n         Line: 102\n         Exact: 0 == 4 ...\n         Message: union should contain 4 items\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_coverage_difference\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_one.py\n       Line number: 30\n       Message: AssertionError: Failed on case with identical coverage reports\n     assert [] == [CoverageItem...covered=True)]\n       \n       Right contains 3 more items, first extra item: CoverageItem(id=1, line='line1', covered=True)\n       \n       Full diff:\n       + []\n       - [\n       -     CoverageItem(id=1, line='line1', covered=True),\n       -     CoverageItem(id=2, line='line2', covered=True),\n       -     CoverageItem(id=3, line='line3', covered=True),\n       - ]\n       Name: tests/test_question_one.py::test_compute_coverage_union\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_one.py\n       Line number: 102\n       Message: AssertionError: union should contain 4 items\n     assert 0 == 4\n      +  where 0 = len([])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 18\n     @pytest.mark.question_one_part_a\n     def test_compute_coverage_difference():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         item1 = CoverageItem(1, \"line1\", True)\n         item2 = CoverageItem(2, \"line2\", True)\n         item3 = CoverageItem(3, \"line3\", True)\n         item4 = CoverageItem(4, \"line4\", True)\n         item5 = CoverageItem(5, \"line5\", True)\n         item6 = CoverageItem(6, \"line6\", True)\n         item7 = CoverageItem(1, \"line1\", False)\n         item8 = CoverageItem(2, \"line2\", False)\n         item9 = CoverageItem(3, \"line3\", False)\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item1, item2, item3]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with identical coverage reports\"\n         assert (\n             compute_coverage_intersection([item1, item2, item3], [item4, item5, item6])\n             == []\n         ), \"Failed on case with no common coverage\"\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item2, item3, item4]\n         ) == [\n             item2,\n             item3,\n         ], \"Failed on case with partial overlap\"\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item3, item2, item1]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with identical coverage reports in different order\"\n         assert (\n             compute_coverage_intersection([], []) == []\n         ), \"Failed on case with empty coverage reports\"\n         assert (\n             compute_coverage_intersection([item1, item2, item3], [item7, item8, item9])\n             == []\n         ), \"Failed on case with same ids but not covered\"\n         assert (\n             compute_coverage_difference([item1, item2, item3], [item1, item2, item3]) == []\n         ), \"Failed on case with identical coverage reports\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item4, item5, item6]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with no common coverage\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item2, item3, item4]\n         ) == [item1], \"Failed on case with partial overlap\"\n         assert (\n             compute_coverage_difference([item1, item2, item3], [item3, item2, item1]) == []\n         ), \"Failed on case with identical coverage reports in different order\"\n         assert (\n             compute_coverage_difference([], []) == []\n         ), \"Failed on case with empty coverage reports\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item7, item8, item9]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with same ids but different coverage status\"\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 89\n     @pytest.mark.question_one_part_a\n     def test_compute_coverage_union():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # standard/easy case: both reports have unique items\n         report_one = [\n             CoverageItem(id=1, line=\"line 1\", covered=True),\n             CoverageItem(id=2, line=\"line 2\", covered=False),\n         ]\n         report_two = [\n             CoverageItem(id=3, line=\"line 3\", covered=True),\n             CoverageItem(id=4, line=\"line 4\", covered=False),\n         ]\n         union_result = compute_coverage_union(report_one, report_two)\n         assert len(union_result) == 4, \"union should contain 4 items\"\n         assert any(\n             item.id == 1 for item in union_result\n         ), \"union should contain item with id 1\"\n         assert any(\n             item.id == 2 for item in union_result\n         ), \"union should contain item with id 2\"\n         assert any(\n             item.id == 3 for item in union_result\n         ), \"union should contain item with id 3\"\n         assert any(\n             item.id == 4 for item in union_result\n         ), \"union should contain item with id 4\"\n         # edge case: both reports have overlapping items\n         report_one = [\n             CoverageItem(id=1, line=\"line 1\", covered=True),\n             CoverageItem(id=2, line=\"line 2\", covered=True),\n         ]\n         report_two = [\n             CoverageItem(id=2, line=\"line 2\", covered=True),\n             CoverageItem(id=3, line=\"line 3\", covered=True),\n         ]\n         union_result = compute_coverage_union(report_one, report_two)\n         assert len(union_result) == 3, \"union should contain 3 items\"\n         assert any(\n             item.id == 1 for item in union_result\n         ), \"union should contain item with id 1\"\n         assert any(\n             item.id == 2 and item.covered for item in union_result\n         ), \"union should contain item with id 2 and covered=True\"\n         assert any(\n             item.id == 3 for item in union_result\n         ), \"union should contain item with id 3\"\n         # edge case: one report is empty\n         report_one = []\n         report_two = [\n             CoverageItem(id=1, line=\"line 1\", covered=True),\n             CoverageItem(id=2, line=\"line 2\", covered=False),\n         ]\n         union_result = compute_coverage_union(report_one, report_two)\n         assert len(union_result) == 2, \"union should contain 2 items\"\n         assert any(\n             item.id == 1 for item in union_result\n         ), \"union should contain item with id 1\"\n         assert any(\n             item.id == 2 for item in union_result\n         ), \"union should contain item with id 2\"\n         # edge case: both reports are empty\n         report_one = []\n         report_two = []\n         union_result = compute_coverage_union(report_one, report_two)\n         assert len(union_result) == 0, \"union should contain 0 items\"\n         # edge case: reports have items with the same id but different coverage status\n         report_one = [\n             CoverageItem(id=1, line=\"line 1\", covered=True),\n         ]\n         report_two = [\n             CoverageItem(id=1, line=\"line 1\", covered=False),\n         ]\n         union_result = compute_coverage_union(report_one, report_two)\n         assert (\n             len(union_result) == 2\n         ), \"union should contain 2 items because coverage status is not same\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Calculate the code coverage score for a test suite."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f5dfc5773b0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_coverage_score - AssertionError: Failed on empty list\n     \n     test_question_one.py::test_compute_coverage_score\n       - Status: Failed\n         Line: 171\n         Exact: 1.0 == 0.0 ...\n         Message: Failed on empty list\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_coverage_score\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_one.py\n       Line number: 171\n       Message: AssertionError: Failed on empty list\n     assert 1.0 == 0.0\n      +  where 1.0 = calculate_coverage_score([])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 167\n     @pytest.mark.question_one_part_b\n     def test_compute_coverage_score():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # test with an empty list\n         assert calculate_coverage_score([]) == 0.0, \"Failed on empty list\"\n         # test with all items covered\n         all_covered = [CoverageItem(1, \"line1\", True) for _ in range(5)]\n         assert calculate_coverage_score(all_covered) == 1.0, \"Failed on all items covered\"\n         # test with no items covered\n         none_covered = [CoverageItem(1, \"line1\", False) for _ in range(5)]\n         assert calculate_coverage_score(none_covered) == 0.0, \"Failed on no items covered\"\n         # test with some items covered\n         some_covered = [\n             CoverageItem(1, \"line1\", True),\n             CoverageItem(2, \"line2\", False),\n             CoverageItem(3, \"line3\", True),\n         ]\n         assert (\n             calculate_coverage_score(some_covered) == 2 / 3\n         ), \"Failed on some items covered\"\n         # test with one item covered\n         one_covered = [CoverageItem(1, \"line1\", True)]\n         assert calculate_coverage_score(one_covered) == 1.0, \"Failed on one item covered\"\n         # test with one item not covered\n         one_not_covered = [CoverageItem(1, \"line1\", False)]\n         assert (\n             calculate_coverage_score(one_not_covered) == 0.0\n         ), \"Failed on one item not covered\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Calculate the mutation score for a test suite."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f2ead3ef5f0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_mutation_score - ZeroDivisionError: division by zero\n     \n     test_question_one.py::test_compute_mutation_score\n       - Status: Failed\n         Line: 193\n         Exact: ZeroDivisionError\n         Message: division by zero\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_mutation_score\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_one.py\n       Line number: 193\n       Message: ZeroDivisionError: division by zero\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 197\n     @pytest.mark.question_one_part_c\n     def test_compute_mutation_score():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # summary of the checks:\n         # check 1: Empty list of mutants\n         # check 2: Empty\n         # check 3: Partially detected\n         # check 4: Fully detected\n         # check 1: Empty list of mutants\n         assert compute_mutation_score([]) == 0.0\n         # check 2: All undetected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", False), Mutant(2, \"line2\", False)])\n             == 0.0\n         )\n         # check 3: Partially detected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", True), Mutant(2, \"line2\", False)])\n             == 0.5\n         )\n         # check 4: All detected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", True), Mutant(2, \"line2\", True)])\n             == 1.0\n         )\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Calculate a mutation score when the project domain allows for equivalent mutants."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_d\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f30a2f75760>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_mutation_score_equivalent_aware - ZeroDivisionError: division by zero\n     \n     test_question_one.py::test_compute_mutation_score_equivalent_aware\n       - Status: Failed\n         Line: 233\n         Exact: ZeroDivisionError\n         Message: division by zero\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_mutation_score_equivalent_aware\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_one.py\n       Line number: 233\n       Message: ZeroDivisionError: division by zero\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 224\n     @pytest.mark.question_one_part_d\n     def test_compute_mutation_score_equivalent_aware():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # check 1: empty list of mutants\n         assert compute_mutation_score_equivalent_aware([]) == 0.0\n         # check 2: all undetected mutants that are not equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", False, False), Mutant(2, \"line2\", False, False)]\n             )\n             == 0.0\n         )\n         # check 3: partially detected mutants that are not equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", True, False), Mutant(2, \"line2\", False, False)]\n             )\n             == 0.5\n         )\n         # check 4: all detected mutants that are not equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", True, False), Mutant(2, \"line2\", True, False)]\n             )\n             == 1.0\n         )\n         # check 5: equivalent mutants should be ignored\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", True, True), Mutant(2, \"line2\", False, True)]\n             )\n             == 0.0\n         )\n         # check 6: mix of detected, undetected, and equivalent mutants\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [\n                     Mutant(1, \"line1\", True, False),\n                     Mutant(2, \"line2\", False, False),\n                     Mutant(3, \"line3\", True, True),\n                     Mutant(4, \"line4\", False, True),\n                 ]\n             )\n             == 0.5\n         )\n         # check 7: all of the mutants are equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [\n                     Mutant(1, \"line1\", True, True),\n                     Mutant(2, \"line2\", False, True),\n                     Mutant(3, \"line3\", True, True),\n                     Mutant(4, \"line4\", False, True),\n                 ]\n             )\n             == 0.0\n         )\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable by implementing an automated linter."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f554fc3b9e0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_extract_prints_from_source_code - AssertionError: Failed to count the number of \n     print functions in the source code\n     FAILED tests/test_question_two.py::test_extract_prints_from_source_code_embedded_print - AssertionError: Failed to count\n     the number of print functions in the source code with a print string\n     FAILED tests/test_question_two.py::test_extract_prints_from_source_code_check_violation - AssertionError: Failed to \n     count the number of print functions in the source code\n     \n     test_question_two.py::test_extract_prints_from_source_code\n       - Status: Failed\n         Line: 73\n         Exact: 0 == 3\n         Message: Failed to count the number of print functions in the source code\n     \n     test_question_two.py::test_extract_prints_from_source_code_embedded_print\n       - Status: Failed\n         Line: 82\n         Exact: 0 == 3\n         Message: Failed to count the number of print functions in the source code with a print string\n     \n     test_question_two.py::test_zero_prints_in_source_code\n       - Status: Passed\n         Line: 91\n         Code: (\n             count == 0\n         )\n         Exact: 0 == 0\n     \n     test_question_two.py::test_zero_prints_in_source_code_embedded\n       - Status: Passed\n         Line: 100\n         Code: (\n             count == 0\n         )\n         Exact: 0 == 0\n     \n     test_question_two.py::test_extract_prints_from_source_code_check_violation\n       - Status: Failed\n         Line: 109\n         Exact: 0 == 3\n         Message: Failed to count the number of print functions in the source code\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_extract_prints_from_source_code\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 73\n       Message: AssertionError: Failed to count the number of print functions in the source code\n     assert 0 == 3\n       Name: tests/test_question_two.py::test_extract_prints_from_source_code_embedded_print\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 82\n       Message: AssertionError: Failed to count the number of print functions in the source code with a print string\n     assert 0 == 3\n       Name: tests/test_question_two.py::test_extract_prints_from_source_code_check_violation\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 109\n       Message: AssertionError: Failed to count the number of print functions in the source code\n     assert 0 == 3\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 69\n     @pytest.mark.question_two_part_a\n     def test_extract_prints_from_source_code():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_print_functions_linter(source_code_one_three_prints)\n         assert (\n             count == 3\n         ), \"Failed to count the number of print functions in the source code\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 78\n     @pytest.mark.question_two_part_a\n     def test_extract_prints_from_source_code_embedded_print():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_print_functions_linter(source_code_one_three_prints_embedded_print)\n         assert (\n             count == 3\n         ), \"Failed to count the number of print functions in the source code with a print string\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 105\n     @pytest.mark.question_two_part_a\n     def test_extract_prints_from_source_code_check_violation():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_print_functions_linter(source_code_one_three_prints)\n         assert (\n             count == 3\n         ), \"Failed to count the number of print functions in the source code\"\n         not_violation = detect_print_function_violation_linter(\n             source_code_one_three_prints, 2\n         )\n         assert not_violation, \"Failed to detect the violation of the print function limit\"\n         expected_violation = detect_print_function_violation_linter(\n             source_code_one_three_prints\n         )\n         assert (\n             expected_violation\n         ), \"Failed to detect the violation of the default print function limit\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Implement an automated analysis of a Python program's abstract syntax tree."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f7ac8bbf1d0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_extract_plus_operator_from_source_code - AssertionError: Failed to count the \n     number of plus operators in the source code (exist)\n     FAILED tests/test_question_two.py::test_extract_minus_operator_from_source_code - AssertionError: Failed to count the \n     number of minus operators in the source code (exist)\n     \n     test_question_two.py::test_extract_plus_operator_from_source_code\n       - Status: Failed\n         Line: 128\n         Exact: 0 == 1\n         Message: Failed to count the number of plus operators in the source code (exist)\n     \n     test_question_two.py::test_extract_minus_operator_from_source_code\n       - Status: Failed\n         Line: 141\n         Exact: 0 == 1\n         Message: Failed to count the number of minus operators in the source code (exist)\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_extract_plus_operator_from_source_code\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 128\n       Message: AssertionError: Failed to count the number of plus operators in the source code (exist)\n     assert 0 == 1\n       Name: tests/test_question_two.py::test_extract_minus_operator_from_source_code\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 141\n       Message: AssertionError: Failed to count the number of minus operators in the source code (exist)\n     assert 0 == 1\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 124\n     @pytest.mark.question_two_part_b\n     def test_extract_plus_operator_from_source_code():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_plus_operator_in_assignments(source_code_two_one_plus)\n         assert (\n             count == 1\n         ), \"Failed to count the number of plus operators in the source code (exist)\"\n         count = count_plus_operator_in_assignments(source_code_two_one_minus)\n         assert (\n             count == 0\n         ), \"Failed to count the number of plus operators in the source code (not exist)\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 137\n     @pytest.mark.question_two_part_b\n     def test_extract_minus_operator_from_source_code():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_minus_operator_in_assignments(source_code_two_one_minus)\n         assert (\n             count == 1\n         ), \"Failed to count the number of minus operators in the source code (exist)\"\n         count = count_minus_operator_in_assignments(source_code_two_one_plus)\n         assert (\n             count == 0\n         ), \"Failed to count the number of minus operators in the source code (not exist)\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Enhance an automated linter so that it is easily configurable."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fd9ff83ec60>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_check_print_violations_with_list - TypeError: \n     check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     FAILED tests/test_question_two.py::test_count_print_violations_with_list - AssertionError: Failed to count the \n     violations of print functions in the list\n     \n     test_question_two.py::test_check_print_violations_with_list\n       - Status: Failed\n         Line: 161\n         Exact: TypeError\n         Message: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     \n     test_question_two.py::test_count_print_violations_with_list\n       - Status: Failed\n         Line: 195\n         Exact: {} == {'\\ndef examp...than 5\"\\n': 0} ...\n         Message: Failed to count the violations of print functions in the list\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_check_print_violations_with_list\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 161\n       Message: TypeError: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n       Name: tests/test_question_two.py::test_count_print_violations_with_list\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 195\n       Message: AssertionError: Failed to count the violations of print functions in the list\n     assert {} == {'\\ndef examp...than 5\"\\n': 0}\n       \n       Right contains 4 more items:\n       {'\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > 5:\\n    \n     print(\"print x is greater than 5\")\\n': 3,\n        '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > 5:\\n    \n     print(\"x is greater than 5\")\\n': 3,\n        '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n': 0,\n        '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n        example = \"print x is greater than \n     5\"\\n': 0}\n       \n       Full diff:\n       + {}\n       - {\n       -     '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > \n     5:\\n        print(\"print x is greater than 5\")\\n': 3,\n       -     '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > \n     5:\\n        print(\"x is greater than 5\")\\n': 3,\n       -     '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n': 0,\n       -     '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n        example = \"print x is greater than\n     5\"\\n': 0,\n       - }\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 150\n     @pytest.mark.question_two_part_c\n     def test_check_print_violations_with_list():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of source codes\n         source_codes = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n             source_code_one_zero_prints,\n             source_code_one_zero_prints_embedded_print,\n         ]\n         # check the violations of print functions in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         expected_violations = {\n             source_code_one_three_prints: True,\n             source_code_one_three_prints_embedded_print: True,\n             source_code_one_zero_prints: False,\n             source_code_one_zero_prints_embedded_print: False,\n         }\n         # assert the results\n         assert (\n             violations == expected_violations\n         ), \"Failed to check the violations of print functions in the list\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 175\n     @pytest.mark.question_two_part_c\n     def test_count_print_violations_with_list():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of source codes\n         source_codes = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n             source_code_one_zero_prints,\n             source_code_one_zero_prints_embedded_print,\n         ]\n         # check the violations of print functions in the list\n         violations = count_print_violations_in_functions(source_codes)\n         # expected results\n         expected_violations = {\n             source_code_one_three_prints: 3,\n             source_code_one_three_prints_embedded_print: 3,\n             source_code_one_zero_prints: 0,\n             source_code_one_zero_prints_embedded_print: 0,\n         }\n         # assert the results\n         assert (\n             violations == expected_violations\n         ), \"Failed to count the violations of print functions in the list\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Enhance an automated linter so that produces easy-to-understand output."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_d\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f70c406f200>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_check_overall_violations - TypeError: check_print_violations_in_functions() \n     missing 1 required positional argument: 'max_prints'\n     FAILED tests/test_question_two.py::test_check_overall_violations_source_code_strings - TypeError: \n     check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     \n     test_question_two.py::test_check_overall_violations\n       - Status: Failed\n         Line: 211\n         Exact: TypeError\n         Message: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     \n     test_question_two.py::test_check_overall_violations_source_code_strings\n       - Status: Failed\n         Line: 242\n         Exact: TypeError\n         Message: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_check_overall_violations\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 211\n       Message: TypeError: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n       Name: tests/test_question_two.py::test_check_overall_violations_source_code_strings\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 242\n       Message: TypeError: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 200\n     @pytest.mark.question_two_part_d\n     def test_check_overall_violations():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of source codes\n         source_codes = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n             source_code_one_zero_prints,\n             source_code_one_zero_prints_embedded_print,\n         ]\n         # check the violations of print functions in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         expected_violations = {\n             source_code_one_three_prints: True,\n             source_code_one_three_prints_embedded_print: True,\n             source_code_one_zero_prints: False,\n             source_code_one_zero_prints_embedded_print: False,\n         }\n         # assert the results\n         assert violations == expected_violations\n         # check the violations of plus operators in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         overall_violations = has_overall_print_violation_linter(violations)\n         # assert the results\n         assert (\n             overall_violations\n         ), \"Failed to check the overall violations of print functions in the list\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 231\n     @pytest.mark.question_two_part_d\n     def test_check_overall_violations_source_code_strings():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of source codes\n         source_codes = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n             source_code_one_zero_prints,\n             source_code_one_zero_prints_embedded_print,\n         ]\n         # check the violations of print functions in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         expected_violations = {\n             source_code_one_three_prints: True,\n             source_code_one_three_prints_embedded_print: True,\n             source_code_one_zero_prints: False,\n             source_code_one_zero_prints_embedded_print: False,\n         }\n         # assert the results\n         assert violations == expected_violations\n         # check the violations of plus operators in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         overall_violations_string_list = get_overall_print_violation_linter(violations)\n         # assert the results\n         assert (\n             overall_violations_string_list\n         ), \"Failed to get a non-None listing of the overall violations\"\n         expected_violations_strings_list = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n         ]\n         assert overall_violations_string_list == expected_violations_strings_list\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Implement functions to perform the automated mutation testing of a Python program."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_three_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fc72ea7b200>\n     \n     Test Trace\n     \n     FAILED tests/test_question_three.py::test_mutate_functions_building_blocks - AssertionError: delete_random_character \n     failed: length mismatch\n     \n     test_question_three.py::test_mutate_functions_building_blocks\n       - Status: Failed\n         Line: 26\n         Exact: 5 == (5 - 1) ...\n         Message: delete_random_character failed: length mismatch\n     \n     test_question_three.py::test_overall_mutation_function\n       - Status: Passed\n         Line: 58\n         Code: len(result) in [\n             len(s) - 1,\n             len(s),\n             len(s) + 1,\n         ]\n         Exact: 7 in [6, 7, 8] ...\n       - Status: Passed\n         Line: 63\n         Code: all(\n             c in string.printable for c in result\n         )\n         Exact: True ...\n       - Status: Passed\n         Line: 69\n         Code: (\n             result == \"\" or len(result) == 1\n         )\n         Exact: ('' == '')\n       - Status: Passed\n         Line: 72\n         Code: all(\n             c in string.printable for c in result\n         )\n         Exact: True ...\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_three.py::test_mutate_functions_building_blocks\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_three.py\n       Line number: 26\n       Message: AssertionError: delete_random_character failed: length mismatch\n     assert 5 == (5 - 1)\n      +  where 5 = len('hello')\n      +  and   5 = len('hello')\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 20\n     @pytest.mark.question_three_part_a\n     def test_mutate_functions_building_blocks():\n         \"\"\"Test for a question part.\"\"\"\n         # test delete_random_character function\n         s = \"hello\"\n         result = delete_random_character(s)\n         assert len(result) == len(s) - 1, \"delete_random_character failed: length mismatch\"\n         assert all(\n             c in s for c in result\n         ), \"delete_random_character failed: unexpected characters\"\n         assert (\n             delete_random_character(\"\") == \"\"\n         ), \"delete_random_character failed: empty string case\"\n         # test insert_random_character function\n         s = \"world\"\n         result = insert_random_character(s)\n         assert len(result) == len(s) + 1, \"insert_random_character failed: length mismatch\"\n         assert all(\n             c in string.printable for c in result\n         ), \"insert_random_character failed: non-printable character inserted\"\n         # test flip_random_character function\n         s = \"test\"\n         result = flip_random_character(s)\n         assert len(result) == len(s), \"flip_random_character failed: length mismatch\"\n         assert any(\n             c != s[i] for i, c in enumerate(result)\n         ), \"flip_random_character failed: no character flipped\"\n         assert (\n             flip_random_character(\"\") == \"\"\n         ), \"flip_random_character failed: empty string case\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Implement domain-specific approaches to determining character string equality."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_three_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fccc88573e0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_three.py::test_equals_for_strings - AssertionError: equals failed: different strings of same \n     length should not be equal\n     FAILED tests/test_question_three.py::test_equals_for_strings_after_mutation - AssertionError: equals failed: identical \n     strings should be equal\n     FAILED tests/test_question_three.py::test_check_multiple_equals_for_mutation - AssertionError: Should return 2 results\n     \n     test_question_three.py::test_equals_for_strings\n       - Status: Passed\n         Line: 81\n         Code: equals_for_mutation(\n             \"hello\", \"hello\"\n         )\n         Exact: True ...\n       - Status: Failed\n         Line: 85\n         Exact: not True ...\n         Message: equals failed: different strings of same length should not be equal\n     \n     test_question_three.py::test_equals_for_strings_after_mutation\n       - Status: Failed\n         Line: 108\n         Exact: not True ...\n         Message: equals failed: identical strings should be equal\n     \n     test_question_three.py::test_check_multiple_equals_for_mutation\n       - Status: Failed\n         Line: 119\n         Exact: 0 == 2 ...\n         Message: Should return 2 results\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_three.py::test_equals_for_strings\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_three.py\n       Line number: 85\n       Message: AssertionError: equals failed: different strings of same length should not be equal\n     assert not True\n      +  where True = equals_for_mutation('hello', 'world')\n       Name: tests/test_question_three.py::test_equals_for_strings_after_mutation\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_three.py\n       Line number: 108\n       Message: AssertionError: equals failed: identical strings should be equal\n     assert not True\n      +  where True = equals_for_mutation('hello', 'hello')\n      +    where 'hello' = mutate('hello')\n       Name: tests/test_question_three.py::test_check_multiple_equals_for_mutation\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_three.py\n       Line number: 119\n       Message: AssertionError: Should return 2 results\n     assert 0 == 2\n      +  where 0 = len([])\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 77\n     @pytest.mark.question_three_part_b\n     def test_equals_for_strings():\n         \"\"\"Test for a question part.\"\"\"\n         # test case: equal strings\n         assert equals_for_mutation(\n             \"hello\", \"hello\"\n         ), \"equals failed: identical strings should be equal\"\n         # test case: different strings of same length\n         assert not equals_for_mutation(\n             \"hello\", \"world\"\n         ), \"equals failed: different strings of same length should not be equal\"\n         # test case: different strings of different lengths\n         assert not equals_for_mutation(\n             \"hello\", \"helloo\"\n         ), \"equals failed: strings of different lengths should not be equal\"\n         # test case: empty strings\n         assert equals_for_mutation(\"\", \"\"), \"equals failed: empty strings should be equal\"\n         # test case: one empty string and one non-empty string\n         assert not equals_for_mutation(\n             \"\", \"nonempty\"\n         ), \"equals failed: empty string and non-empty string should not be equal\"\n         # test case: case sensitivity\n         assert not equals_for_mutation(\n             \"Hello\", \"hello\"\n         ), \"equals failed: strings with different cases should not be equal\"\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 104\n     @pytest.mark.question_three_part_b\n     def test_equals_for_strings_after_mutation():\n         \"\"\"Test for a question part.\"\"\"\n         # test case: equal strings\n         assert not equals_for_mutation(\n             \"hello\", mutate(\"hello\")\n         ), \"equals failed: identical strings should be equal\"\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 113\n     @pytest.mark.question_three_part_b\n     def test_check_multiple_equals_for_mutation():\n         \"\"\"Test for a question part.\"\"\"\n         # test case with equal strings\n         values = [(\"test\", \"test\"), (\"example\", \"example\")]\n         result = check_multiple_equals_for_mutation(values)\n         assert len(result) == 2, \"Should return 2 results\"\n         assert result[0] == (\"test\", \"test\", True), \"First pair should be equal\"\n         assert result[1] == (\"example\", \"example\", True), \"Second pair should be equal\"\n         # test case with different strings\n         values = [(\"test\", \"Test\"), (\"example\", \"Example\")]\n         result = check_multiple_equals_for_mutation(values)\n         assert len(result) == 2, \"Should return 2 results\"\n         assert result[0] == (\"test\", \"Test\", False), \"First pair should not be equal\"\n         assert result[1] == (\"example\", \"Example\", False), \"Second pair should not be equal\"\n         # test case with mixed strings\n         values = [(\"test\", \"test\"), (\"example\", \"Example\")]\n         result = check_multiple_equals_for_mutation(values)\n         assert len(result) == 2, \"Should return 2 results\"\n         assert result[0] == (\"test\", \"test\", True), \"First pair should be equal\"\n         assert result[1] == (\"example\", \"Example\", False), \"Second pair should not be equal\"\n         # test case with empty strings\n         values = [(\"\", \"\"), (\"\", \"non-empty\")]\n         result = check_multiple_equals_for_mutation(values)\n         assert len(result) == 2, \"Should return 2 results\"\n         assert result[0] == (\"\", \"\", True), \"First pair should be equal\"\n         assert result[1] == (\"\", \"non-empty\", False), \"Second pair should not be equal\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Enhance a string equality function so that it is general-purpose and easy to maintain."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_three_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fe7919bf470>\n     \n     Test Trace\n     \n     FAILED tests/test_question_three.py::test_check_multiple_string_equality - KeyError: ''\n     \n     test_question_three.py::test_check_multiple_string_equality\n       - Status: Passed\n         Line: 149\n         Code: all(\n             result.values()\n         )\n         Exact: True ...\n       - Status: Passed\n         Line: 157\n         Code: not any(\n             result.values()\n         )\n         Exact: not False ...\n       - Status: Passed\n         Line: 164\n         Code: all(\n             result.values()\n         )\n         Exact: True ...\n       - Status: Failed\n         Line: 171\n         Exact: KeyError\n         Message: ''\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_three.py::test_check_multiple_string_equality\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_three.py\n       Line number: 171\n       Message: KeyError: ''\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 142\n     @pytest.mark.question_three_part_c\n     def test_check_multiple_string_equality():\n         \"\"\"Test for a question part.\"\"\"\n         # test case: all strings equal to comparison string\n         input_strings = [\"test\", \"test\", \"test\"]\n         comparison_string = \"test\"\n         result = check_multiple_string_equality(input_strings, comparison_string)\n         assert all(\n             result.values()\n         ), \"check_multiple_string_equality failed: all strings should be equal to comparison string\"\n     \n         # test case: no strings equal to comparison string\n         input_strings = [\"test1\", \"test2\", \"test3\"]\n         comparison_string = \"exemplify\"\n         result = check_multiple_string_equality(input_strings, comparison_string)\n         assert not any(\n             result.values()\n         ), \"check_multiple_string_equality failed: no strings should be equal to comparison string\"\n         # test case: empty strings\n         input_strings = [\"\", \"\"]\n         comparison_string = \"\"\n         result = check_multiple_string_equality(input_strings, comparison_string)\n         assert all(\n             result.values()\n         ), \"check_multiple_string_equality failed: empty strings should be equal to empty comparison string\"\n         # test case: one empty string and one non-empty string\n         input_strings = [\"\", \"nonempty\"]\n         comparison_string = \"\"\n         result = check_multiple_string_equality(input_strings, comparison_string)\n         assert result[\n             \"\"\n         ], \"check_multiple_string_equality failed: empty string should be equal to empty comparison string\"\n         assert not result[\n             \"nonempty\"\n         ], \"check_multiple_string_equality failed: non-empty string should not be equal to empty comparison string\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Implement a function that can perform automated fuzzing through input generation."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_three_part_d\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f7b1413ef60>\n     \n     Test Trace\n     \n     FAILED tests/test_question_three.py::test_generate_fuzzer_values - ValueError: empty range in randrange(32, 0)\n     \n     test_question_three.py::test_generate_fuzzer_values\n       - Status: Failed\n         Line: 319\n         Exact: ValueError\n         Message: empty range in randrange(32, 0)\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_three.py::test_generate_fuzzer_values\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_three.py\n       Line number: 319\n       Message: ValueError: empty range in randrange(32, 0)\n     \n     Failing Test\n     \n     # File: tests/test_question_three.py Line: 179\n     @pytest.mark.question_three_part_d\n     def test_generate_fuzzer_values():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         max_length = 10\n         result = generate_fuzzer_values(max_length)\n         assert len(result) <= max_length, \"Generated string is too long\"\n         char_start = 65\n         char_range = 26\n         result = generate_fuzzer_values(100, char_start, char_range)\n         for char in result:\n             assert (\n                 char_start <= ord(char) < char_start + char_range\n             ), \"Character is not in range\"\n         result = generate_fuzzer_values(0)\n         assert result == \"\", \"Empty string not generated\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 4 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_four_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Implement an approach to determining if functions are approved after code review."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_four_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f3e3c7b6c60>\n     \n     Test Trace\n     \n     FAILED tests/test_question_four.py::test_get_approved_functions - AssertionError: Should return 2 approved functions\n     FAILED tests/test_question_four.py::test_get_not_approved_functions - AssertionError: Should return 2 not approved \n     functions\n     \n     test_question_four.py::test_get_approved_functions\n       - Status: Failed\n         Line: 34\n         Exact: 0 == 2 ...\n         Message: Should return 2 approved functions\n     \n     test_question_four.py::test_get_not_approved_functions\n       - Status: Failed\n         Line: 68\n         Exact: 0 == 2 ...\n         Message: Should return 2 not approved functions\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_four.py::test_get_approved_functions\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_four.py\n       Line number: 34\n       Message: AssertionError: Should return 2 approved functions\n     assert 0 == 2\n      +  where 0 = len([])\n       Name: tests/test_question_four.py::test_get_not_approved_functions\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_four.py\n       Line number: 68\n       Message: AssertionError: Should return 2 not approved functions\n     assert 0 == 2\n      +  where 0 = len([])\n     \n     Failing Test\n     \n     # File: tests/test_question_four.py Line: 19\n     @pytest.mark.question_four_part_a\n     def test_get_approved_functions():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of function details\n         functions = [\n             FunctionDetails(id=1, name=\"func1\", approved=True),\n             FunctionDetails(id=2, name=\"func2\", approved=False),\n             FunctionDetails(id=3, name=\"func3\", approved=True),\n             FunctionDetails(id=4, name=\"func4\", approved=False),\n         ]\n         # create a module with the list of functions\n         module = Module(functions=functions)\n         # get the approved functions\n         approved_functions = get_approved_functions(module)\n         # check the number of approved functions\n         assert len(approved_functions) == 2, \"Should return 2 approved functions\"\n         # check the first approved function\n         assert approved_functions[0].id == 1, \"First approved function should have id 1\"\n         assert (\n             approved_functions[0].name == \"func1\"\n         ), \"First approved function should have name 'func1'\"\n         assert (\n             approved_functions[0].approved is True\n         ), \"First approved function should be approved\"\n         # check the second approved function\n         assert approved_functions[1].id == 3, \"Second approved function should have id 3\"\n         assert (\n             approved_functions[1].name == \"func3\"\n         ), \"Second approved function should have name 'func3'\"\n         assert (\n             approved_functions[1].approved is True\n         ), \"Second approved function should be approved\"\n     \n     Failing Test\n     \n     # File: tests/test_question_four.py Line: 53\n     @pytest.mark.question_four_part_a\n     def test_get_not_approved_functions():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of function details\n         functions = [\n             FunctionDetails(id=1, name=\"func1\", approved=True),\n             FunctionDetails(id=2, name=\"func2\", approved=False),\n             FunctionDetails(id=3, name=\"func3\", approved=True),\n             FunctionDetails(id=4, name=\"func4\", approved=False),\n         ]\n         # create a module with the list of functions\n         module = Module(functions=functions)\n         # get the approved functions\n         not_approved_functions = get_not_approved_functions(module)\n         # check the number of not approved functions\n         assert len(not_approved_functions) == 2, \"Should return 2 not approved functions\"\n         # check the first not approved function\n         assert (\n             not_approved_functions[0].id == 2\n         ), \"First not approved function should have id 2\"\n         assert (\n             not_approved_functions[0].name == \"func2\"\n         ), \"First not approved function should have name 'func2'\"\n         assert (\n             not_approved_functions[0].approved is False\n         ), \"First not approved function should not be approved\"\n         # check the second not approved function\n         assert (\n             not_approved_functions[1].id == 4\n         ), \"Second not approved function should have id 4\"\n         assert (\n             not_approved_functions[1].name == \"func4\"\n         ), \"Second not approved function should have name 'func4'\"\n         assert (\n             not_approved_functions[1].approved is False\n         ), \"Second not approved function should not be approved\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 4 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_four_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Implement functions to determine if a module looks good to merge after code review."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_four_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f48ebc7b2f0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_four.py::test_determine_if_module_looks_good_to_merge_yes_way - AssertionError: Module should\n     look good to merge\n     \n     test_question_four.py::test_determine_if_module_looks_good_to_merge_no_way\n       - Status: Passed\n         Line: 105\n         Code: not_merge_candidate is False\n         Exact: False is False\n     \n     test_question_four.py::test_determine_if_module_looks_good_to_merge_yes_way\n       - Status: Failed\n         Line: 122\n         Exact: False is True\n         Message: Module should look good to merge\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_four.py::test_determine_if_module_looks_good_to_merge_yes_way\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_four.py\n       Line number: 122\n       Message: AssertionError: Module should look good to merge\n     assert False is True\n     \n     Failing Test\n     \n     # File: tests/test_question_four.py Line: 108\n     @pytest.mark.question_four_part_b\n     def test_determine_if_module_looks_good_to_merge_yes_way():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of function details\n         functions = [\n             FunctionDetails(id=1, name=\"func1\", approved=True),\n             FunctionDetails(id=2, name=\"func2\", approved=True),\n             FunctionDetails(id=3, name=\"func3\", approved=True),\n             FunctionDetails(id=4, name=\"func4\", approved=True),\n         ]\n         # create a module with the list of functions\n         module = Module(functions=functions)\n         # get the approved functions\n         not_merge_candidate = determine_looks_good_to_merge_human_approval(module)\n         assert not_merge_candidate is True, \"Module should look good to merge\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 4 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_four_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Extract details (e.g., mix, max, avg) about the cyclomatic complexity of functions."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_four_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fd7f4e673e0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_four.py::test_cyclomatic_complexity_functions - AssertionError: Minimum cyclomatic complexity\n     should be 2\n     \n     test_question_four.py::test_cyclomatic_complexity_functions\n       - Status: Failed\n         Line: 138\n         Exact: 10 == 2\n         Message: Minimum cyclomatic complexity should be 2\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_four.py::test_cyclomatic_complexity_functions\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_four.py\n       Line number: 138\n       Message: AssertionError: Minimum cyclomatic complexity should be 2\n     assert 10 == 2\n     \n     Failing Test\n     \n     # File: tests/test_question_four.py Line: 125\n     @pytest.mark.question_four_part_c\n     def test_cyclomatic_complexity_functions():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of function details with cyclomatic complexity\n         functions = [\n             (1, \"func1\", 5),\n             (2, \"func2\", 3),\n             (3, \"func3\", 7),\n             (4, \"func4\", 2),\n             (5, \"func5\", 4),\n         ]\n         # test minimum cyclomatic complexity\n         min_complexity = calculate_minimum_cyclomatic_complexity(functions)\n         assert min_complexity == 2, \"Minimum cyclomatic complexity should be 2\"\n         # test average cyclomatic complexity\n         avg_complexity = calculate_average_cyclomatic_complexity(functions)\n         assert avg_complexity == pytest.approx(\n             4.2, 0.01\n         ), \"Average cyclomatic complexity should be 4.2\"\n         # test maximum cyclomatic complexity\n         max_complexity = calculate_maximum_cyclomatic_complexity(functions)\n         assert max_complexity == 7, \"Maximum cyclomatic complexity should be 7\"\n         # test with an empty list\n         empty_functions = []\n         with pytest.raises(ValueError, match=\"The list of functions is empty\"):\n             calculate_minimum_cyclomatic_complexity(empty_functions)\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 4 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_four_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Determine whether or not it is acceptable to merge a pull request based on complexity."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_four_part_d\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f8bfaa67170>\n     \n     Test Trace\n     \n     FAILED tests/test_question_four.py::test_cyclomatic_complexity_functions_approved_for_merge - AssertionError: Cannot \n     merge when the threshold is set to 1\n     \n     test_question_four.py::test_cyclomatic_complexity_functions_approved_for_merge\n       - Status: Failed\n         Line: 165\n         Exact: not True\n         Message: Cannot merge when the threshold is set to 1\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_four.py::test_cyclomatic_complexity_functions_approved_for_merge\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_four.py\n       Line number: 165\n       Message: AssertionError: Cannot merge when the threshold is set to 1\n     assert not True\n     \n     Failing Test\n     \n     # File: tests/test_question_four.py Line: 153\n     @pytest.mark.question_four_part_d\n     def test_cyclomatic_complexity_functions_approved_for_merge():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of function details with cyclomatic complexity\n         functions = [\n             (1, \"func1\", 5),\n             (2, \"func2\", 3),\n             (3, \"func3\", 7),\n             (4, \"func4\", 2),\n             (5, \"func5\", 4),\n         ]\n         merge = determine_looks_good_to_merge_cyclomatic_complexity(functions, 1)\n         assert not merge, \"Cannot merge when the threshold is set to 1\"\n         merge = determine_looks_good_to_merge_cyclomatic_complexity(functions, 7)\n         assert not merge, \"Cannot merge when the threshold is set to 7\"\n         merge = determine_looks_good_to_merge_cyclomatic_complexity(functions, 8)\n         assert merge, \"Can merge when the threshold is set to 8\"\n         merge = determine_looks_good_to_merge_cyclomatic_complexity(functions, 10)\n         assert merge, \"Can merge when the threshold is set to 10\"\n         # test with an empty list\n         empty_functions = []\n         with pytest.raises(ValueError, match=\"The list of functions is empty\"):\n             determine_looks_good_to_merge_cyclomatic_complexity(empty_functions, 1000)\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Ensure that Question 1 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_one.py", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it adheres to linting rules."}]}}, "status": false, "diagnostic": "questions/question_one.py:94:5: D103 Missing docstring in public function\n        |\n     94 | def compute_coverage_intersection(\n        |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     95 |     coverage_report_one: List[CoverageItem], coverage_report_two: List[CoverageItem]\n     96 | ) -> List[CoverageItem]:\n        |\n     \n     questions/question_one.py:101:5: D103 Missing docstring in public function\n         |\n     101 | def compute_coverage_difference(\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     102 |     coverage_report_one: List[CoverageItem], coverage_report_two: List[CoverageItem]\n     103 | ) -> List[CoverageItem]:\n         |\n     \n     questions/question_one.py:108:5: D103 Missing docstring in public function\n         |\n     108 | def compute_coverage_union(\n         |     ^^^^^^^^^^^^^^^^^^^^^^ D103\n     109 |     coverage_report_one: List[CoverageItem], coverage_report_two: List[CoverageItem]\n     110 | ) -> List[CoverageItem]:\n         |\n     \n     questions/question_one.py:141:5: D103 Missing docstring in public function\n         |\n     141 | def calculate_coverage_score(coverage_items):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     142 |     return 1.0\n         |\n     \n     questions/question_one.py:230:5: D103 Missing docstring in public function\n         |\n     230 | def compute_mutation_score_equivalent_aware(mutants):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     231 |     detected_mutants = 0\n     232 |     total_mutants = 0\n         |\n     \n     Found 5 errors."}, {"description": "Ensure that Question 1 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_one.py --check", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it is formatted correctly."}]}}, "status": false, "diagnostic": "Would reformat: questions/question_one.py\n     1 file would be reformatted"}, {"description": "Ensure that Question 1 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_one.py", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": false, "diagnostic": "questions/question_one.py:97: error: Need type annotation for \"coverage_intersection\" (hint: \"coverage_intersection: list[<type>] = ...\")  [var-annotated]\n     questions/question_one.py:104: error: Need type annotation for \"coverage_difference\" (hint: \"coverage_difference: list[<type>] = ...\")  [var-annotated]\n     questions/question_one.py:111: error: Need type annotation for \"coverage_union\" (hint: \"coverage_union: list[<type>] = ...\")  [var-annotated]\n     Found 3 errors in 1 file (checked 1 source file)"}, {"description": "Ensure that Question 1 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_one.py --count", "fragment": 6, "count": 1, "exact": true}, "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 1 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_one.py --count", "fragment": 8, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because it uses correct docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 1 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_one.py --count", "fragment": 0, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because all functions have docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 2 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_two.py", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it adheres to linting rules."}]}}, "status": false, "diagnostic": "questions/question_two.py:8:8: F401 [*] `ast` imported but unused\n       |\n     6 | # to the industry best practices for Python source code.\n     7 | \n     8 | import ast\n       |        ^^^ F401\n     9 | from typing import Dict, List\n       |\n       = help: Remove unused import: `ast`\n     \n     questions/question_two.py:9:20: F401 [*] `typing.Dict` imported but unused\n        |\n      8 | import ast\n      9 | from typing import Dict, List\n        |                    ^^^^ F401\n     10 | \n     11 | # Introduction: Read This First! {{{\n        |\n        = help: Remove unused import\n     \n     questions/question_two.py:9:26: F401 [*] `typing.List` imported but unused\n        |\n      8 | import ast\n      9 | from typing import Dict, List\n        |                          ^^^^ F401\n     10 | \n     11 | # Introduction: Read This First! {{{\n        |\n        = help: Remove unused import\n     \n     questions/question_two.py:75:5: D103 Missing docstring in public function\n        |\n     75 | def count_print_functions_linter(source_code):\n        |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     76 |     print_count = 0\n     77 |     return print_count\n        |\n     \n     questions/question_two.py:80:5: D103 Missing docstring in public function\n        |\n     80 | def detect_print_function_violation_linter(source_code, max_prints):\n        |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     81 |     return False\n        |\n     \n     questions/question_two.py:140:5: D103 Missing docstring in public function\n         |\n     138 | # engineer using it.\n     139 | \n     140 | def count_operator_in_assignments(source_code: str, operator_type: type) -> int:\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     141 |     operator_count = 0\n     142 |     return operator_count\n         |\n     \n     questions/question_two.py:145:5: D103 Missing docstring in public function\n         |\n     145 | def count_plus_operator_in_assignments(source_code: str) -> int:\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     146 |     return 0\n         |\n     \n     questions/question_two.py:149:5: D103 Missing docstring in public function\n         |\n     149 | def count_minus_operator_in_assignments(source_code: str) -> int:\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     150 |     return 0\n         |\n     \n     questions/question_two.py:195:5: D103 Missing docstring in public function\n         |\n     193 | # using it.\n     194 | \n     195 | def check_print_violations_in_functions(source_code_list, max_prints):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     196 |     violations_dict = {}\n     197 |     return violations_dict\n         |\n     \n     questions/question_two.py:200:5: D103 Missing docstring in public function\n         |\n     200 | def count_print_violations_in_functions(source_code_list):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     201 |     violations_dict_count_mapping = {}\n     202 |     return violations_dict_count_mapping\n         |\n     \n     questions/question_two.py:249:5: D103 Missing docstring in public function\n         |\n     249 | def get_overall_print_violation_linter(violations_dict):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     250 |     return [\"\"]\n         |\n     \n     questions/question_two.py:253:5: D103 Missing docstring in public function\n         |\n     253 | def has_overall_print_violation_linter(violations_dict):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     254 |     return False\n         |\n     \n     Found 12 errors.\n     [*] 3 fixable with the `--fix` option."}, {"description": "Ensure that Question 2 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_two.py --check", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it is formatted correctly."}]}}, "status": false, "diagnostic": "Would reformat: questions/question_two.py\n     1 file would be reformatted"}, {"description": "Ensure that Question 2 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_two.py", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": true}, {"description": "Ensure that Question 2 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_two.py --count", "fragment": 9, "count": 1, "exact": true}, "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 2 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_two.py --count", "fragment": 9, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because it uses correct docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 2 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_two.py --count", "fragment": 0, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because all functions have docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 3 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_three.py", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it adheres to linting rules."}]}}, "status": false, "diagnostic": "questions/question_three.py:9:32: F401 [*] `typing.Tuple` imported but unused\n        |\n      8 | import random\n      9 | from typing import Dict, List, Tuple\n        |                                ^^^^^ F401\n     10 | \n     11 | from questions import constants\n        |\n        = help: Remove unused import: `typing.Tuple`\n     \n     questions/question_three.py:11:23: F401 [*] `questions.constants` imported but unused\n        |\n      9 | from typing import Dict, List, Tuple\n     10 | \n     11 | from questions import constants\n        |                       ^^^^^^^^^ F401\n     12 | \n     13 | # Introduction: Read This First! {{{\n        |\n        = help: Remove unused import: `questions.constants`\n     \n     questions/question_three.py:70:5: D103 Missing docstring in public function\n        |\n     70 | def delete_random_character(s):\n        |     ^^^^^^^^^^^^^^^^^^^^^^^ D103\n     71 |     return s\n        |\n     \n     questions/question_three.py:74:5: D103 Missing docstring in public function\n        |\n     74 | def insert_random_character(s):\n        |     ^^^^^^^^^^^^^^^^^^^^^^^ D103\n     75 |     return s\n        |\n     \n     questions/question_three.py:78:5: D103 Missing docstring in public function\n        |\n     78 | def flip_random_character(s):\n        |     ^^^^^^^^^^^^^^^^^^^^^ D103\n     79 |     return s\n        |\n     \n     questions/question_three.py:82:5: D103 Missing docstring in public function\n        |\n     82 | def mutate(s):\n        |     ^^^^^^ D103\n     83 |     return s\n        |\n     \n     questions/question_three.py:124:5: D103 Missing docstring in public function\n         |\n     122 | # engineer using it.\n     123 | \n     124 | def equals_for_mutation(one, two):\n         |     ^^^^^^^^^^^^^^^^^^^ D103\n     125 |     return True\n         |\n     \n     questions/question_three.py:128:5: D103 Missing docstring in public function\n         |\n     128 | def check_multiple_equals_for_mutation(values):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     129 |     # create the list to store the results\n     130 |     results = []\n         |\n     \n     questions/question_three.py:196:5: D103 Missing docstring in public function\n         |\n     196 | def generate_fuzzer_values(\n         |     ^^^^^^^^^^^^^^^^^^^^^^ D103\n     197 |     max_length: int = 100, char_start: int = 32, char_range: int = 32\n     198 | ):\n         |\n     \n     Found 9 errors.\n     [*] 2 fixable with the `--fix` option."}, {"description": "Ensure that Question 3 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_three.py --check", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it is formatted correctly."}]}}, "status": false, "diagnostic": "Would reformat: questions/question_three.py\n     1 file would be reformatted"}, {"description": "Ensure that Question 3 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_three.py", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": false, "diagnostic": "questions/question_three.py:163: error: Need type annotation for \"equality_dict\" (hint: \"equality_dict: dict[<type>, <type>] = ...\")  [var-annotated]\n     Found 1 error in 1 file (checked 1 source file)"}, {"description": "Ensure that Question 3 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_three.py --count", "fragment": 8, "count": 1, "exact": true}, "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 3 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_three.py --count", "fragment": 8, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because it uses correct docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 3 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_three.py --count", "fragment": 0, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because all functions have docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 4 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_four.py", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it adheres to linting rules."}]}}, "status": false, "diagnostic": "questions/question_four.py:8:26: F401 [*] `typing.Tuple` imported but unused\n        |\n      6 | # to the industry best practices for Python source code.\n      7 | \n      8 | from typing import List, Tuple\n        |                          ^^^^^ F401\n      9 | \n     10 | # Introduction: Read This First! {{{\n        |\n        = help: Remove unused import: `typing.Tuple`\n     \n     questions/question_four.py:98:5: D103 Missing docstring in public function\n        |\n     98 | def get_approved_functions(module: Module) -> List[FunctionDetails]:\n        |     ^^^^^^^^^^^^^^^^^^^^^^ D103\n     99 |     return []\n        |\n     \n     questions/question_four.py:102:5: D103 Missing docstring in public function\n         |\n     102 | def get_not_approved_functions(module: Module) -> List[FunctionDetails]:\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     103 |     return []\n         |\n     \n     questions/question_four.py:125:5: D103 Missing docstring in public function\n         |\n     125 | def determine_looks_good_to_merge_human_approval(module):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     126 |     total_functions = len(module.functions)\n     127 |     approved_functions = []\n         |\n     \n     questions/question_four.py:166:5: D103 Missing docstring in public function\n         |\n     166 | def calculate_minimum_cyclomatic_complexity(functions):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     167 |     return 10\n         |\n     \n     questions/question_four.py:170:5: D103 Missing docstring in public function\n         |\n     170 | def calculate_average_cyclomatic_complexity(functions):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     171 |     return 10.0\n         |\n     \n     questions/question_four.py:174:5: D103 Missing docstring in public function\n         |\n     174 | def calculate_maximum_cyclomatic_complexity(functions):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     175 |     return 100.0\n         |\n     \n     questions/question_four.py:206:5: D103 Missing docstring in public function\n         |\n     206 | def determine_looks_good_to_merge_cyclomatic_complexity(functions, threshold):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     207 |     # this is like the person who approves every pull\n     208 |     # request without carefully looking at the complexity of the code!\n         |\n     \n     Found 8 errors.\n     [*] 1 fixable with the `--fix` option."}, {"description": "Ensure that Question 4 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_four.py --check", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it is formatted correctly."}]}}, "status": true}, {"description": "Ensure that Question 4 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_four.py", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": true}, {"description": "Ensure that Question 4 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_four.py --count", "fragment": 7, "count": 1, "exact": true}, "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 4 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_four.py --count", "fragment": 9, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because it uses correct docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 4 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_four.py --count", "fragment": 0, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because all functions have docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}]}