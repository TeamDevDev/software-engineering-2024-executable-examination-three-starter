{"amount_correct": 39, "percentage_score": 60, "report_time": "2024-12-11 12:28:15", "checks": [{"description": "Ensure that the README.md file exists inside of the root of the GitHub repository", "check": "ConfirmFileExists", "status": true, "path": "../README.md"}, {"description": "Delete the phrase 'Add Your Name Here' and add your own name as an Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "Add Your Name Here", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 2 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for README.md", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 5 fragment(s) in the README.md or the output while expecting exactly 0"}, {"description": "Retype the every word in the Honor Code pledge in README.md", "check": "MatchFileFragment", "options": {"fragment": "I adhered to the Allegheny College Honor Code while completing this executable examination.", "count": 3, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 2 fragment(s) in the README.md or the output while expecting exactly 3"}, {"description": "Indicate that you have completed all of the tasks in the README.md", "check": "MatchFileFragment", "options": {"fragment": "- [X]", "count": 11, "exact": true}, "status": false, "path": "../README.md", "diagnostic": "Found 0 fragment(s) in the README.md or the output while expecting exactly 11"}, {"description": "Ensure that question_one.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_one.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_one.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_one.py", "diagnostic": "Found 11 fragment(s) in the question_one.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_one.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 10, "exact": false}, "status": true, "path": "questions/question_one.py"}, {"description": "Create a sufficient number of single-line comments in question_one.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_one.py"}, {"description": "Ensure that test_question_one.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_one.py"}, {"description": "Ensure that question_two.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_two.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_two.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": false, "path": "questions/question_two.py", "diagnostic": "Found 11 fragment(s) in the question_two.py or the output while expecting exactly 0"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_two.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 8, "exact": false}, "status": false, "path": "questions/question_two.py", "diagnostic": "Found 1 comment(s) in the question_two.py or the output"}, {"description": "Create a sufficient number of single-line comments in question_two.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_two.py"}, {"description": "Ensure that test_question_two.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_two.py"}, {"description": "Ensure that question_three.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_three.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_three.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": true, "path": "questions/question_three.py"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_three.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 8, "exact": false}, "status": true, "path": "questions/question_three.py"}, {"description": "Create a sufficient number of single-line comments in question_three.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_three.py"}, {"description": "Ensure that test_question_two.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_three.py"}, {"description": "Ensure that question_four.py file exists in the questions/ directory", "check": "ConfirmFileExists", "status": true, "path": "questions/question_four.py"}, {"description": "Complete all TODOs, remove the TODO markers, and rewrite comments for question_four.py", "check": "MatchFileFragment", "options": {"fragment": "TODO", "count": 0, "exact": true}, "status": true, "path": "questions/question_four.py"}, {"description": "Create a sufficient number of docstring (i.e., multiple-line) comments in question_four.py", "check": "CountMultipleLineComments", "options": {"language": "Python", "count": 10, "exact": false}, "status": true, "path": "questions/question_four.py"}, {"description": "Create a sufficient number of single-line comments in question_four.py", "check": "CountSingleLineComments", "options": {"language": "Python", "count": 20, "exact": false}, "status": true, "path": "questions/question_four.py"}, {"description": "Ensure that test_question_four.py file exists in the tests/ directory", "check": "ConfirmFileExists", "status": true, "path": "tests/test_question_four.py"}, {"description": "Run checks for Question 1 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Automatically analyze test coverage reports through set-theoretic approaches."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f6b47afb4d0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_coverage_difference - AssertionError: Failed on case with identical \n     coverage reports\n     FAILED tests/test_question_one.py::test_compute_coverage_union - AssertionError: union should contain 4 items\n     \n     test_question_one.py::test_compute_coverage_difference\n       - Status: Failed\n         Line: 30\n         Exact: [] == [CoverageItem...covered=True)] ...\n         Message: Failed on case with identical coverage reports\n     \n     test_question_one.py::test_compute_coverage_union\n       - Status: Failed\n         Line: 102\n         Exact: 0 == 4 ...\n         Message: union should contain 4 items\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_coverage_difference\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_one.py\n       Line number: 30\n       Message: AssertionError: Failed on case with identical coverage reports\n     assert [] == [CoverageItem...covered=True)]\n       \n       Right contains 3 more items, first extra item: CoverageItem(id=1, line='line1', covered=True)\n       \n       Full diff:\n       + []\n       - [\n       -     CoverageItem(id=1, line='line1', covered=True),\n       -     CoverageItem(id=2, line='line2', covered=True),\n       -     CoverageItem(id=3, line='line3', covered=True),\n       - ]\n       Name: tests/test_question_one.py::test_compute_coverage_union\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_one.py\n       Line number: 102\n       Message: AssertionError: union should contain 4 items\n     assert 0 == 4\n      +  where 0 = len([])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 18\n     @pytest.mark.question_one_part_a\n     def test_compute_coverage_difference():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         item1 = CoverageItem(1, \"line1\", True)\n         item2 = CoverageItem(2, \"line2\", True)\n         item3 = CoverageItem(3, \"line3\", True)\n         item4 = CoverageItem(4, \"line4\", True)\n         item5 = CoverageItem(5, \"line5\", True)\n         item6 = CoverageItem(6, \"line6\", True)\n         item7 = CoverageItem(1, \"line1\", False)\n         item8 = CoverageItem(2, \"line2\", False)\n         item9 = CoverageItem(3, \"line3\", False)\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item1, item2, item3]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with identical coverage reports\"\n         assert (\n             compute_coverage_intersection([item1, item2, item3], [item4, item5, item6])\n             == []\n         ), \"Failed on case with no common coverage\"\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item2, item3, item4]\n         ) == [\n             item2,\n             item3,\n         ], \"Failed on case with partial overlap\"\n         assert compute_coverage_intersection(\n             [item1, item2, item3], [item3, item2, item1]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with identical coverage reports in different order\"\n         assert (\n             compute_coverage_intersection([], []) == []\n         ), \"Failed on case with empty coverage reports\"\n         assert (\n             compute_coverage_intersection([item1, item2, item3], [item7, item8, item9])\n             == []\n         ), \"Failed on case with same ids but not covered\"\n         assert (\n             compute_coverage_difference([item1, item2, item3], [item1, item2, item3]) == []\n         ), \"Failed on case with identical coverage reports\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item4, item5, item6]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with no common coverage\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item2, item3, item4]\n         ) == [item1], \"Failed on case with partial overlap\"\n         assert (\n             compute_coverage_difference([item1, item2, item3], [item3, item2, item1]) == []\n         ), \"Failed on case with identical coverage reports in different order\"\n         assert (\n             compute_coverage_difference([], []) == []\n         ), \"Failed on case with empty coverage reports\"\n         assert compute_coverage_difference(\n             [item1, item2, item3], [item7, item8, item9]\n         ) == [\n             item1,\n             item2,\n             item3,\n         ], \"Failed on case with same ids but different coverage status\"\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 89\n     @pytest.mark.question_one_part_a\n     def test_compute_coverage_union():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # standard/easy case: both reports have unique items\n         report_one = [\n             CoverageItem(id=1, line=\"line 1\", covered=True),\n             CoverageItem(id=2, line=\"line 2\", covered=False),\n         ]\n         report_two = [\n             CoverageItem(id=3, line=\"line 3\", covered=True),\n             CoverageItem(id=4, line=\"line 4\", covered=False),\n         ]\n         union_result = compute_coverage_union(report_one, report_two)\n         assert len(union_result) == 4, \"union should contain 4 items\"\n         assert any(\n             item.id == 1 for item in union_result\n         ), \"union should contain item with id 1\"\n         assert any(\n             item.id == 2 for item in union_result\n         ), \"union should contain item with id 2\"\n         assert any(\n             item.id == 3 for item in union_result\n         ), \"union should contain item with id 3\"\n         assert any(\n             item.id == 4 for item in union_result\n         ), \"union should contain item with id 4\"\n         # edge case: both reports have overlapping items\n         report_one = [\n             CoverageItem(id=1, line=\"line 1\", covered=True),\n             CoverageItem(id=2, line=\"line 2\", covered=True),\n         ]\n         report_two = [\n             CoverageItem(id=2, line=\"line 2\", covered=True),\n             CoverageItem(id=3, line=\"line 3\", covered=True),\n         ]\n         union_result = compute_coverage_union(report_one, report_two)\n         assert len(union_result) == 3, \"union should contain 3 items\"\n         assert any(\n             item.id == 1 for item in union_result\n         ), \"union should contain item with id 1\"\n         assert any(\n             item.id == 2 and item.covered for item in union_result\n         ), \"union should contain item with id 2 and covered=True\"\n         assert any(\n             item.id == 3 for item in union_result\n         ), \"union should contain item with id 3\"\n         # edge case: one report is empty\n         report_one = []\n         report_two = [\n             CoverageItem(id=1, line=\"line 1\", covered=True),\n             CoverageItem(id=2, line=\"line 2\", covered=False),\n         ]\n         union_result = compute_coverage_union(report_one, report_two)\n         assert len(union_result) == 2, \"union should contain 2 items\"\n         assert any(\n             item.id == 1 for item in union_result\n         ), \"union should contain item with id 1\"\n         assert any(\n             item.id == 2 for item in union_result\n         ), \"union should contain item with id 2\"\n         # edge case: both reports are empty\n         report_one = []\n         report_two = []\n         union_result = compute_coverage_union(report_one, report_two)\n         assert len(union_result) == 0, \"union should contain 0 items\"\n         # edge case: reports have items with the same id but different coverage status\n         report_one = [\n             CoverageItem(id=1, line=\"line 1\", covered=True),\n         ]\n         report_two = [\n             CoverageItem(id=1, line=\"line 1\", covered=False),\n         ]\n         union_result = compute_coverage_union(report_one, report_two)\n         assert (\n             len(union_result) == 2\n         ), \"union should contain 2 items because coverage status is not same\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Calculate the code coverage score for a test suite."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f9ec8e7f0b0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_coverage_score - AssertionError: Failed on empty list\n     \n     test_question_one.py::test_compute_coverage_score\n       - Status: Failed\n         Line: 171\n         Exact: 1.0 == 0.0 ...\n         Message: Failed on empty list\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_coverage_score\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_one.py\n       Line number: 171\n       Message: AssertionError: Failed on empty list\n     assert 1.0 == 0.0\n      +  where 1.0 = calculate_coverage_score([])\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 167\n     @pytest.mark.question_one_part_b\n     def test_compute_coverage_score():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # test with an empty list\n         assert calculate_coverage_score([]) == 0.0, \"Failed on empty list\"\n         # test with all items covered\n         all_covered = [CoverageItem(1, \"line1\", True) for _ in range(5)]\n         assert calculate_coverage_score(all_covered) == 1.0, \"Failed on all items covered\"\n         # test with no items covered\n         none_covered = [CoverageItem(1, \"line1\", False) for _ in range(5)]\n         assert calculate_coverage_score(none_covered) == 0.0, \"Failed on no items covered\"\n         # test with some items covered\n         some_covered = [\n             CoverageItem(1, \"line1\", True),\n             CoverageItem(2, \"line2\", False),\n             CoverageItem(3, \"line3\", True),\n         ]\n         assert (\n             calculate_coverage_score(some_covered) == 2 / 3\n         ), \"Failed on some items covered\"\n         # test with one item covered\n         one_covered = [CoverageItem(1, \"line1\", True)]\n         assert calculate_coverage_score(one_covered) == 1.0, \"Failed on one item covered\"\n         # test with one item not covered\n         one_not_covered = [CoverageItem(1, \"line1\", False)]\n         assert (\n             calculate_coverage_score(one_not_covered) == 0.0\n         ), \"Failed on one item not covered\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Calculate the mutation score for a test suite."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7fa3d9a78d70>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_mutation_score - ZeroDivisionError: division by zero\n     \n     test_question_one.py::test_compute_mutation_score\n       - Status: Failed\n         Line: 193\n         Exact: ZeroDivisionError\n         Message: division by zero\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_mutation_score\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_one.py\n       Line number: 193\n       Message: ZeroDivisionError: division by zero\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 197\n     @pytest.mark.question_one_part_c\n     def test_compute_mutation_score():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # summary of the checks:\n         # check 1: Empty list of mutants\n         # check 2: Empty\n         # check 3: Partially detected\n         # check 4: Fully detected\n         # check 1: Empty list of mutants\n         assert compute_mutation_score([]) == 0.0\n         # check 2: All undetected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", False), Mutant(2, \"line2\", False)])\n             == 0.0\n         )\n         # check 3: Partially detected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", True), Mutant(2, \"line2\", False)])\n             == 0.5\n         )\n         # check 4: All detected mutants\n         assert (\n             compute_mutation_score([Mutant(1, \"line1\", True), Mutant(2, \"line2\", True)])\n             == 1.0\n         )\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 1 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_one_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Calculate a mutation score when the project domain allows for equivalent mutants."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_one_part_d\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f6637177170>\n     \n     Test Trace\n     \n     FAILED tests/test_question_one.py::test_compute_mutation_score_equivalent_aware - ZeroDivisionError: division by zero\n     \n     test_question_one.py::test_compute_mutation_score_equivalent_aware\n       - Status: Failed\n         Line: 233\n         Exact: ZeroDivisionError\n         Message: division by zero\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_one.py::test_compute_mutation_score_equivalent_aware\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_one.py\n       Line number: 233\n       Message: ZeroDivisionError: division by zero\n     \n     Failing Test\n     \n     # File: tests/test_question_one.py Line: 224\n     @pytest.mark.question_one_part_d\n     def test_compute_mutation_score_equivalent_aware():\n         \"\"\"Confirm correctness of question part.\"\"\"\n         # check 1: empty list of mutants\n         assert compute_mutation_score_equivalent_aware([]) == 0.0\n         # check 2: all undetected mutants that are not equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", False, False), Mutant(2, \"line2\", False, False)]\n             )\n             == 0.0\n         )\n         # check 3: partially detected mutants that are not equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", True, False), Mutant(2, \"line2\", False, False)]\n             )\n             == 0.5\n         )\n         # check 4: all detected mutants that are not equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", True, False), Mutant(2, \"line2\", True, False)]\n             )\n             == 1.0\n         )\n         # check 5: equivalent mutants should be ignored\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [Mutant(1, \"line1\", True, True), Mutant(2, \"line2\", False, True)]\n             )\n             == 0.0\n         )\n         # check 6: mix of detected, undetected, and equivalent mutants\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [\n                     Mutant(1, \"line1\", True, False),\n                     Mutant(2, \"line2\", False, False),\n                     Mutant(3, \"line3\", True, True),\n                     Mutant(4, \"line4\", False, True),\n                 ]\n             )\n             == 0.5\n         )\n         # check 7: all of the mutants are equivalent\n         assert (\n             compute_mutation_score_equivalent_aware(\n                 [\n                     Mutant(1, \"line1\", True, True),\n                     Mutant(2, \"line2\", False, True),\n                     Mutant(3, \"line3\", True, True),\n                     Mutant(4, \"line4\", False, True),\n                 ]\n             )\n             == 0.0\n         )\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable by implementing an automated linter."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_a\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f55966ff680>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_extract_prints_from_source_code - AssertionError: Failed to count the number of \n     print functions in the source code\n     FAILED tests/test_question_two.py::test_extract_prints_from_source_code_embedded_print - AssertionError: Failed to count\n     the number of print functions in the source code with a print string\n     FAILED tests/test_question_two.py::test_extract_prints_from_source_code_check_violation - AssertionError: Failed to \n     count the number of print functions in the source code\n     \n     test_question_two.py::test_extract_prints_from_source_code\n       - Status: Failed\n         Line: 73\n         Exact: 0 == 3\n         Message: Failed to count the number of print functions in the source code\n     \n     test_question_two.py::test_extract_prints_from_source_code_embedded_print\n       - Status: Failed\n         Line: 82\n         Exact: 0 == 3\n         Message: Failed to count the number of print functions in the source code with a print string\n     \n     test_question_two.py::test_zero_prints_in_source_code\n       - Status: Passed\n         Line: 91\n         Code: (\n             count == 0\n         )\n         Exact: 0 == 0\n     \n     test_question_two.py::test_zero_prints_in_source_code_embedded\n       - Status: Passed\n         Line: 100\n         Code: (\n             count == 0\n         )\n         Exact: 0 == 0\n     \n     test_question_two.py::test_extract_prints_from_source_code_check_violation\n       - Status: Failed\n         Line: 109\n         Exact: 0 == 3\n         Message: Failed to count the number of print functions in the source code\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_extract_prints_from_source_code\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 73\n       Message: AssertionError: Failed to count the number of print functions in the source code\n     assert 0 == 3\n       Name: tests/test_question_two.py::test_extract_prints_from_source_code_embedded_print\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 82\n       Message: AssertionError: Failed to count the number of print functions in the source code with a print string\n     assert 0 == 3\n       Name: tests/test_question_two.py::test_extract_prints_from_source_code_check_violation\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 109\n       Message: AssertionError: Failed to count the number of print functions in the source code\n     assert 0 == 3\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 69\n     @pytest.mark.question_two_part_a\n     def test_extract_prints_from_source_code():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_print_functions_linter(source_code_one_three_prints)\n         assert (\n             count == 3\n         ), \"Failed to count the number of print functions in the source code\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 78\n     @pytest.mark.question_two_part_a\n     def test_extract_prints_from_source_code_embedded_print():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_print_functions_linter(source_code_one_three_prints_embedded_print)\n         assert (\n             count == 3\n         ), \"Failed to count the number of print functions in the source code with a print string\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 105\n     @pytest.mark.question_two_part_a\n     def test_extract_prints_from_source_code_check_violation():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_print_functions_linter(source_code_one_three_prints)\n         assert (\n             count == 3\n         ), \"Failed to count the number of print functions in the source code\"\n         not_violation = detect_print_function_violation_linter(\n             source_code_one_three_prints, 2\n         )\n         assert not_violation, \"Failed to detect the violation of the print function limit\"\n         expected_violation = detect_print_function_violation_linter(\n             source_code_one_three_prints\n         )\n         assert (\n             expected_violation\n         ), \"Failed to detect the violation of the default print function limit\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Implement an automated analysis of a Python program's abstract syntax tree."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_b\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f160711c4a0>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_extract_plus_operator_from_source_code - AssertionError: Failed to count the \n     number of plus operators in the source code (exist)\n     FAILED tests/test_question_two.py::test_extract_minus_operator_from_source_code - AssertionError: Failed to count the \n     number of minus operators in the source code (exist)\n     \n     test_question_two.py::test_extract_plus_operator_from_source_code\n       - Status: Failed\n         Line: 128\n         Exact: 0 == 1\n         Message: Failed to count the number of plus operators in the source code (exist)\n     \n     test_question_two.py::test_extract_minus_operator_from_source_code\n       - Status: Failed\n         Line: 141\n         Exact: 0 == 1\n         Message: Failed to count the number of minus operators in the source code (exist)\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_extract_plus_operator_from_source_code\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 128\n       Message: AssertionError: Failed to count the number of plus operators in the source code (exist)\n     assert 0 == 1\n       Name: tests/test_question_two.py::test_extract_minus_operator_from_source_code\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 141\n       Message: AssertionError: Failed to count the number of minus operators in the source code (exist)\n     assert 0 == 1\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 124\n     @pytest.mark.question_two_part_b\n     def test_extract_plus_operator_from_source_code():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_plus_operator_in_assignments(source_code_two_one_plus)\n         assert (\n             count == 1\n         ), \"Failed to count the number of plus operators in the source code (exist)\"\n         count = count_plus_operator_in_assignments(source_code_two_one_minus)\n         assert (\n             count == 0\n         ), \"Failed to count the number of plus operators in the source code (not exist)\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 137\n     @pytest.mark.question_two_part_b\n     def test_extract_minus_operator_from_source_code():\n         \"\"\"Test for a question part.\"\"\"\n         count = count_minus_operator_in_assignments(source_code_two_one_minus)\n         assert (\n             count == 1\n         ), \"Failed to count the number of minus operators in the source code (exist)\"\n         count = count_minus_operator_in_assignments(source_code_two_one_plus)\n         assert (\n             count == 0\n         ), \"Failed to count the number of minus operators in the source code (not exist)\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Enhance an automated linter so that it is easily configurable."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_c\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f791e957170>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_check_print_violations_with_list - TypeError: \n     check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     FAILED tests/test_question_two.py::test_count_print_violations_with_list - AssertionError: Failed to count the \n     violations of print functions in the list\n     \n     test_question_two.py::test_check_print_violations_with_list\n       - Status: Failed\n         Line: 161\n         Exact: TypeError\n         Message: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     \n     test_question_two.py::test_count_print_violations_with_list\n       - Status: Failed\n         Line: 195\n         Exact: {} == {'\\ndef examp...than 5\"\\n': 0} ...\n         Message: Failed to count the violations of print functions in the list\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_check_print_violations_with_list\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 161\n       Message: TypeError: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n       Name: tests/test_question_two.py::test_count_print_violations_with_list\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 195\n       Message: AssertionError: Failed to count the violations of print functions in the list\n     assert {} == {'\\ndef examp...than 5\"\\n': 0}\n       \n       Right contains 4 more items:\n       {'\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > 5:\\n    \n     print(\"print x is greater than 5\")\\n': 3,\n        '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > 5:\\n    \n     print(\"x is greater than 5\")\\n': 3,\n        '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n': 0,\n        '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n        example = \"print x is greater than \n     5\"\\n': 0}\n       \n       Full diff:\n       + {}\n       - {\n       -     '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > \n     5:\\n        print(\"print x is greater than 5\")\\n': 3,\n       -     '\\ndef example_function():\\n    print(\"Hello, World!\")\\n    print(\"This is a test.\")\\n    x = 10\\n    if x > \n     5:\\n        print(\"x is greater than 5\")\\n': 3,\n       -     '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n': 0,\n       -     '\\ndef example_function():\\n    x = 10\\n    if x > 5:\\n        x = 5\\n        example = \"print x is greater than\n     5\"\\n': 0,\n       - }\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 150\n     @pytest.mark.question_two_part_c\n     def test_check_print_violations_with_list():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of source codes\n         source_codes = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n             source_code_one_zero_prints,\n             source_code_one_zero_prints_embedded_print,\n         ]\n         # check the violations of print functions in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         expected_violations = {\n             source_code_one_three_prints: True,\n             source_code_one_three_prints_embedded_print: True,\n             source_code_one_zero_prints: False,\n             source_code_one_zero_prints_embedded_print: False,\n         }\n         # assert the results\n         assert (\n             violations == expected_violations\n         ), \"Failed to check the violations of print functions in the list\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 175\n     @pytest.mark.question_two_part_c\n     def test_count_print_violations_with_list():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of source codes\n         source_codes = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n             source_code_one_zero_prints,\n             source_code_one_zero_prints_embedded_print,\n         ]\n         # check the violations of print functions in the list\n         violations = count_print_violations_in_functions(source_codes)\n         # expected results\n         expected_violations = {\n             source_code_one_three_prints: 3,\n             source_code_one_three_prints_embedded_print: 3,\n             source_code_one_zero_prints: 0,\n             source_code_one_zero_prints_embedded_print: 0,\n         }\n         # assert the results\n         assert (\n             violations == expected_violations\n         ), \"Failed to count the violations of print functions in the list\"\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 2 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_two_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Enhance an automated linter so that produces easy-to-understand output."}]}}, "status": false, "diagnostic": "Parameter Information\n     \n     - project: .\n     - tests: tests\n     - report: [<ReportType.exitcode: 'status'>, <ReportType.testtrace: 'trace'>, <ReportType.testfailures: 'failure'>, \n     <ReportType.setup: 'setup'>, <ReportType.testcodes: 'code'>]\n     - mark: question_two_part_d\n     - maxfail: 10\n     - advice_method: AdviceMethod.api_key\n     - advice_model: None\n     - advice_server: None\n     - debug: False\n     - fancy: False\n     - syntax_theme: Theme.ansi_dark\n     - return_code: 0\n     - litellm_thread: <Thread(Thread-1 (load_litellm), initial)>\n     - display_report_type: ReportType.testadvice\n     - json_report_plugin: <pytest_jsonreport.plugin.JSONReport object at 0x7f9e40153500>\n     \n     Test Trace\n     \n     FAILED tests/test_question_two.py::test_check_overall_violations - TypeError: check_print_violations_in_functions() \n     missing 1 required positional argument: 'max_prints'\n     FAILED tests/test_question_two.py::test_check_overall_violations_source_code_strings - TypeError: \n     check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     \n     test_question_two.py::test_check_overall_violations\n       - Status: Failed\n         Line: 211\n         Exact: TypeError\n         Message: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     \n     test_question_two.py::test_check_overall_violations_source_code_strings\n       - Status: Failed\n         Line: 242\n         Exact: TypeError\n         Message: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     \n     Test Failure(s)\n     \n       Name: tests/test_question_two.py::test_check_overall_violations\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 211\n       Message: TypeError: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n       Name: tests/test_question_two.py::test_check_overall_violations_source_code_strings\n       Path: <...>/software-engineering-2024-executable-examination-three-starter/exam/tests/test_question_two.py\n       Line number: 242\n       Message: TypeError: check_print_violations_in_functions() missing 1 required positional argument: 'max_prints'\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 200\n     @pytest.mark.question_two_part_d\n     def test_check_overall_violations():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of source codes\n         source_codes = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n             source_code_one_zero_prints,\n             source_code_one_zero_prints_embedded_print,\n         ]\n         # check the violations of print functions in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         expected_violations = {\n             source_code_one_three_prints: True,\n             source_code_one_three_prints_embedded_print: True,\n             source_code_one_zero_prints: False,\n             source_code_one_zero_prints_embedded_print: False,\n         }\n         # assert the results\n         assert violations == expected_violations\n         # check the violations of plus operators in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         overall_violations = has_overall_print_violation_linter(violations)\n         # assert the results\n         assert (\n             overall_violations\n         ), \"Failed to check the overall violations of print functions in the list\"\n     \n     Failing Test\n     \n     # File: tests/test_question_two.py Line: 231\n     @pytest.mark.question_two_part_d\n     def test_check_overall_violations_source_code_strings():\n         \"\"\"Test for a question part.\"\"\"\n         # create a list of source codes\n         source_codes = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n             source_code_one_zero_prints,\n             source_code_one_zero_prints_embedded_print,\n         ]\n         # check the violations of print functions in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         expected_violations = {\n             source_code_one_three_prints: True,\n             source_code_one_three_prints_embedded_print: True,\n             source_code_one_zero_prints: False,\n             source_code_one_zero_prints_embedded_print: False,\n         }\n         # assert the results\n         assert violations == expected_violations\n         # check the violations of plus operators in the list\n         violations = check_print_violations_in_functions(source_codes)\n         # expected results\n         overall_violations_string_list = get_overall_print_violation_linter(violations)\n         # assert the results\n         assert (\n             overall_violations_string_list\n         ), \"Failed to get a non-None listing of the overall violations\"\n         expected_violations_strings_list = [\n             source_code_one_three_prints,\n             source_code_one_three_prints_embedded_print,\n         ]\n         assert overall_violations_string_list == expected_violations_strings_list\n     \n     Overall Status\n     \n     \u2718 One or more checks failed."}, {"description": "Run checks for Question 3 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Implement functions to perform the automated mutation testing of a Python program."}]}}, "status": true}, {"description": "Run checks for Question 3 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Implement domain-specific approaches to determining character string equality."}]}}, "status": true}, {"description": "Run checks for Question 3 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Enhance a string equality function so that it is general-purpose and easy to maintain."}]}}, "status": true}, {"description": "Run checks for Question 3 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_three_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Implement a function that can perform automated fuzzing through input generation."}]}}, "status": true}, {"description": "Run checks for Question 4 Part (a) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_four_part_a\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Implement an approach to determining if functions are approved after code review."}]}}, "status": true}, {"description": "Run checks for Question 4 Part (b) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_four_part_b\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Implement functions to determine if a module looks good to merge after code review."}]}}, "status": true}, {"description": "Run checks for Question 4 Part (c) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_four_part_c\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO4": {"degree": [{"SE": "D", "rationale": "Extract details (e.g., mix, max, avg) about the cyclomatic complexity of functions."}]}}, "status": true}, {"description": "Run checks for Question 4 Part (d) with 'execexam' command and confirm correct exit code", "command": "poetry run execexam . tests/ --mark \"question_four_part_d\" --no-fancy --report status --report trace --report failure --report setup --report code", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Determine whether or not it is acceptable to merge a pull request based on complexity."}]}}, "status": true}, {"description": "Ensure that Question 1 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_one.py", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it adheres to linting rules."}]}}, "status": false, "diagnostic": "questions/question_one.py:94:5: D103 Missing docstring in public function\n        |\n     94 | def compute_coverage_intersection(\n        |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     95 |     coverage_report_one: List[CoverageItem], coverage_report_two: List[CoverageItem]\n     96 | ) -> List[CoverageItem]:\n        |\n     \n     questions/question_one.py:101:5: D103 Missing docstring in public function\n         |\n     101 | def compute_coverage_difference(\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     102 |     coverage_report_one: List[CoverageItem], coverage_report_two: List[CoverageItem]\n     103 | ) -> List[CoverageItem]:\n         |\n     \n     questions/question_one.py:108:5: D103 Missing docstring in public function\n         |\n     108 | def compute_coverage_union(\n         |     ^^^^^^^^^^^^^^^^^^^^^^ D103\n     109 |     coverage_report_one: List[CoverageItem], coverage_report_two: List[CoverageItem]\n     110 | ) -> List[CoverageItem]:\n         |\n     \n     questions/question_one.py:141:5: D103 Missing docstring in public function\n         |\n     141 | def calculate_coverage_score(coverage_items):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     142 |     return 1.0\n         |\n     \n     questions/question_one.py:230:5: D103 Missing docstring in public function\n         |\n     230 | def compute_mutation_score_equivalent_aware(mutants):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     231 |     detected_mutants = 0\n     232 |     total_mutants = 0\n         |\n     \n     Found 5 errors."}, {"description": "Ensure that Question 1 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_one.py --check", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it is formatted correctly."}]}}, "status": false, "diagnostic": "Would reformat: questions/question_one.py\n     1 file would be reformatted"}, {"description": "Ensure that Question 1 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_one.py", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": false, "diagnostic": "questions/question_one.py:97: error: Need type annotation for \"coverage_intersection\" (hint: \"coverage_intersection: list[<type>] = ...\")  [var-annotated]\n     questions/question_one.py:104: error: Need type annotation for \"coverage_difference\" (hint: \"coverage_difference: list[<type>] = ...\")  [var-annotated]\n     questions/question_one.py:111: error: Need type annotation for \"coverage_union\" (hint: \"coverage_union: list[<type>] = ...\")  [var-annotated]\n     Found 3 errors in 1 file (checked 1 source file)"}, {"description": "Ensure that Question 1 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_one.py --count", "fragment": 6, "count": 1, "exact": true}, "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 1 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_one.py --count", "fragment": 8, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because it uses correct docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 1 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_one.py --count", "fragment": 0, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because all functions have docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 2 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_two.py", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it adheres to linting rules."}]}}, "status": false, "diagnostic": "questions/question_two.py:8:8: F401 [*] `ast` imported but unused\n       |\n     6 | # to the industry best practices for Python source code.\n     7 | \n     8 | import ast\n       |        ^^^ F401\n     9 | from typing import Dict, List\n       |\n       = help: Remove unused import: `ast`\n     \n     questions/question_two.py:9:20: F401 [*] `typing.Dict` imported but unused\n        |\n      8 | import ast\n      9 | from typing import Dict, List\n        |                    ^^^^ F401\n     10 | \n     11 | # Introduction: Read This First! {{{\n        |\n        = help: Remove unused import\n     \n     questions/question_two.py:9:26: F401 [*] `typing.List` imported but unused\n        |\n      8 | import ast\n      9 | from typing import Dict, List\n        |                          ^^^^ F401\n     10 | \n     11 | # Introduction: Read This First! {{{\n        |\n        = help: Remove unused import\n     \n     questions/question_two.py:75:5: D103 Missing docstring in public function\n        |\n     75 | def count_print_functions_linter(source_code):\n        |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     76 |     print_count = 0\n     77 |     return print_count\n        |\n     \n     questions/question_two.py:80:5: D103 Missing docstring in public function\n        |\n     80 | def detect_print_function_violation_linter(source_code, max_prints):\n        |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     81 |     return False\n        |\n     \n     questions/question_two.py:140:5: D103 Missing docstring in public function\n         |\n     138 | # engineer using it.\n     139 | \n     140 | def count_operator_in_assignments(source_code: str, operator_type: type) -> int:\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     141 |     operator_count = 0\n     142 |     return operator_count\n         |\n     \n     questions/question_two.py:145:5: D103 Missing docstring in public function\n         |\n     145 | def count_plus_operator_in_assignments(source_code: str) -> int:\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     146 |     return 0\n         |\n     \n     questions/question_two.py:149:5: D103 Missing docstring in public function\n         |\n     149 | def count_minus_operator_in_assignments(source_code: str) -> int:\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     150 |     return 0\n         |\n     \n     questions/question_two.py:195:5: D103 Missing docstring in public function\n         |\n     193 | # using it.\n     194 | \n     195 | def check_print_violations_in_functions(source_code_list, max_prints):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     196 |     violations_dict = {}\n     197 |     return violations_dict\n         |\n     \n     questions/question_two.py:200:5: D103 Missing docstring in public function\n         |\n     200 | def count_print_violations_in_functions(source_code_list):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     201 |     violations_dict_count_mapping = {}\n     202 |     return violations_dict_count_mapping\n         |\n     \n     questions/question_two.py:249:5: D103 Missing docstring in public function\n         |\n     249 | def get_overall_print_violation_linter(violations_dict):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     250 |     return [\"\"]\n         |\n     \n     questions/question_two.py:253:5: D103 Missing docstring in public function\n         |\n     253 | def has_overall_print_violation_linter(violations_dict):\n         |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ D103\n     254 |     return False\n         |\n     \n     Found 12 errors.\n     [*] 3 fixable with the `--fix` option."}, {"description": "Ensure that Question 2 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_two.py --check", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it is formatted correctly."}]}}, "status": false, "diagnostic": "Would reformat: questions/question_two.py\n     1 file would be reformatted"}, {"description": "Ensure that Question 2 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_two.py", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": true}, {"description": "Ensure that Question 2 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_two.py --count", "fragment": 9, "count": 1, "exact": true}, "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 2 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_two.py --count", "fragment": 9, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because it uses correct docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 2 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_two.py --count", "fragment": 0, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because all functions have docstrings."}]}}, "status": false, "diagnostic": "Found 0 fragment(s) in the file or the output while expecting exactly 1"}, {"description": "Ensure that Question 3 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_three.py", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it adheres to linting rules."}]}}, "status": true}, {"description": "Ensure that Question 3 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_three.py --check", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it is formatted correctly."}]}}, "status": true}, {"description": "Ensure that Question 3 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_three.py", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": true}, {"description": "Ensure that Question 3 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_three.py --count", "fragment": 8, "count": 1, "exact": true}, "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": true}, {"description": "Ensure that Question 3 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_three.py --count", "fragment": 8, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because it uses correct docstrings."}]}}, "status": true}, {"description": "Ensure that Question 3 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_three.py --count", "fragment": 0, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because all functions have docstrings."}]}}, "status": true}, {"description": "Ensure that Question 4 follows industry-standard rules using the command 'ruff check'", "command": "poetry run ruff check questions/question_four.py", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it adheres to linting rules."}]}}, "status": true}, {"description": "Ensure that Question 4 adheres to an industry-standard format using the command 'ruff format'", "command": "poetry run ruff format questions/question_four.py --check", "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Ensure that a software system is maintainable because it is formatted correctly."}]}}, "status": true}, {"description": "Ensure that Question 4 has correct type annotations using the command 'mypy'", "command": "poetry run mypy questions/question_four.py", "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": true}, {"description": "Ensure that Question 4 has correct number of fully type annotated functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --fully-typed -f questions/question_four.py --count", "fragment": 7, "count": 1, "exact": true}, "objectives": {"CLO2": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program meets a domain's standards for type annotations."}]}}, "status": true}, {"description": "Ensure that Question 4 has correct number of documented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --documented -f questions/question_four.py --count", "fragment": 9, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because it uses correct docstrings."}]}}, "status": true}, {"description": "Ensure that Question 4 has no undocumented functions using the command 'symbex'", "check": "MatchCommandFragment", "options": {"command": "poetry run symbex -s --undocumented -f questions/question_four.py --count", "fragment": 0, "count": 1, "exact": true}, "objectives": {"CLO1": {"degree": [{"SE": "D", "rationale": "Confirm that a Python program is maintainable because all functions have docstrings."}]}}, "status": true}]}